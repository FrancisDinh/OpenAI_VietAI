{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import os\n",
    "import glob\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\" ## To configure OpenAI API\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" ## To configure langchain connections with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../content/\"\n",
    "loader = DirectoryLoader(dir_path)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=256\n",
    "chunk_overlap=20\n",
    "text_splitter = MarkdownTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "upf_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only chunk longer than 1 sentence, 10 words\n",
    "filtered_upf_splits = []\n",
    "for chunk in upf_splits:\n",
    "    if len(chunk.page_content.split()) > 5:\n",
    "        filtered_upf_splits.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "for item in filtered_upf_splits:\n",
    "    chunks.append(item.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "knowledge_base = FAISS.from_texts(chunks[:10], embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Confronting the dangers of ultra-processed food\\n\\nA cocktail of additives and preservatives poses a risk to people’s health'),\n",
       "  0.25893632),\n",
       " (Document(page_content='Mr van Tulleken, a doctor and television presenter, draws a distinction between “ultra-processed food” (upf) and “processed food”. Almost everything people consume is processed in some form: rice is harvested and hulled, animals are butchered. He uses a'),\n",
       "  0.2813555),\n",
       " (Document(page_content='there is much to cheer about calories being cheap and abundant, when for most of human history they were neither. But as Chris van Tulleken’s new book, “Ultra-Processed People”, explains, that cheapness and abundance come at a cost.'),\n",
       "  0.32375494),\n",
       " (Document(page_content='nutrient-poor, upf contributes to obesity in part because its palatability and soft texture foster overconsumption, overriding satiety signals from the brain.'),\n",
       "  0.34942997)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledge_base.similarity_search_with_score(\"what is ultra-processed food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Mr van Tulleken, a doctor and television presenter, draws a distinction between “ultra-processed food” (upf) and “processed food”. Almost everything people consume is processed in some form: rice is harvested and hulled, animals are butchered. He uses a'),\n",
       "  0.32386637),\n",
       " (Document(page_content='Confronting the dangers of ultra-processed food\\n\\nA cocktail of additives and preservatives poses a risk to people’s health'),\n",
       "  0.33703643),\n",
       " (Document(page_content='there is much to cheer about calories being cheap and abundant, when for most of human history they were neither. But as Chris van Tulleken’s new book, “Ultra-Processed People”, explains, that cheapness and abundance come at a cost.'),\n",
       "  0.39965206),\n",
       " (Document(page_content='and technology”. A pizza made from scratch contains minimally processed food (wheat turned into flour, tomatoes into sauce, milk into cheese). The one in the freezer, with its thiamine mononitrate and sodium phosphate, is upf.'),\n",
       "  0.4261606)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.similarity_search_with_score(\"what is ultra processed food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1828340610.py, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[80], line 51\u001b[0;36m\u001b[0m\n\u001b[0;31m    return data = loader.load()\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Knowledge_base():\n",
    "    def __init__(self, knowledge_base, threshold=0.5):\n",
    "        self.threshold=threshold\n",
    "        self.knowledge_base = knowledge_base\n",
    "    \n",
    "    def search_doc_from_knowledge_base(self,knowledge_base, question):\n",
    "        # Return the closet doc to question\n",
    "        docs = self.knowledge_base.similarity_search_with_score(question)\n",
    "        closest_doc = self._get_closet_doc_from_docs(docs)\n",
    "        if closest_doc:\n",
    "            return [closest_doc]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Only return closest doc\n",
    "    def _get_closet_doc_from_docs(self, docs):\n",
    "        # Return the doc having the min score below threshold\n",
    "        min_score = self.threshold\n",
    "        min_id = -1\n",
    "        for id, item in enumerate(docs):\n",
    "            doc, score = item\n",
    "            if min_score > score:\n",
    "                min_id = id\n",
    "                min_score = score\n",
    "        if min_score < threshold:\n",
    "            return docs[min_id][0]  # return doc\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "class Text_processor():\n",
    "    def __init__(self, folder_path, chunk_size=256, chunk_overlap=20):\n",
    "        self.folder_path = folder_path\n",
    "        self.folder_list = self._get_recursive_folder()\n",
    "        self.doc_list = self._get_doc_path()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.embeddings=OpenAIEmbeddings()\n",
    "        self.data = None\n",
    "        self.knowledge_base=None\n",
    "    \n",
    "    def _get_rescursive_folder(self):\n",
    "        for folder in glob.iglob(f\"../{self.folder_path}/**\"):\n",
    "            self.folder_list.append(folder)\n",
    "    \n",
    "    def _get_doc_path(self):\n",
    "        for filename in glob.iglob(f'../{self.folder_path}/**/*.md', recursive=True):\n",
    "            self.doc_list.append(filename)\n",
    "    \n",
    "    def _load_docs(self):\n",
    "        loader = DirectoryLoader(self.dir_path)\n",
    "        data = loader.load()\n",
    "        return data\n",
    "        \n",
    "    def _split_docs(self):\n",
    "        # Split the text into chunks using Langchain's CharacterTextSplitter\n",
    "        text_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        upf_splits = text_splitter.split_documents(self.data)\n",
    "        return upf_splits\n",
    "    \n",
    "    def embed_docs(self):\n",
    "        # Load docs\n",
    "        self.data = self._load_docs()\n",
    "        \n",
    "        # Split docs\n",
    "        self.upf_splits = self._split_docs()\n",
    "        \n",
    "        # Filter chunks shorter than 1 sentence or 10 words\n",
    "        self.upf_splits = self._filter_chunk(self)\n",
    "        \n",
    "        # Embed chunks\n",
    "        self.knowledge_base = FAISS.from_texts(documents=self.upf_splits, embedding=self.embeddings)\n",
    "        return Knowledge_base(self.knowledge_base)\n",
    "\n",
    "    def _filter_chunk(self):\n",
    "        filtered_upf_splits = []\n",
    "        for chunk in self.upf_splits:\n",
    "            if len(chunk.page_content.split()) > 5:\n",
    "                filtered_upf_splits.append(chunk)\n",
    "        return filtered_upf_splits\n",
    "    \n",
    "    def _get_doc_len(self, text, threshold_char=10):\n",
    "        # Check if the split is longer than 1 sentences\n",
    "        return len(text.split()) >= threshold_char\n",
    "\n",
    "    def get_n_doc(self):\n",
    "        return len(self.doc_list)\n",
    "\n",
    "    def save_knowledge_base(self, output_path):\n",
    "        self.knowledge_base.save_local(output_path)\n",
    "    \n",
    "    def load_knowledge_base(self, input_path):\n",
    "        self.knowledge_base.load_local(input_path, self.embeddings)\n",
    "        return Knowledge_base(self.knowledge_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
