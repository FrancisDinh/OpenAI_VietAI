{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain import FAISS\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "import os\n",
    "import glob\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\" ## To configure OpenAI API\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\" ## To configure langchain connections with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Knowledge_base():\n",
    "    def __init__(self, knowledge_base, threshold=0.8):\n",
    "        self.threshold=threshold\n",
    "        self.knowledge_base = knowledge_base\n",
    "    \n",
    "    def search_from_knowledge_base(self, question):\n",
    "        # Return the closet doc to question\n",
    "        docs = self.knowledge_base.similarity_search_with_score(question)\n",
    "        closest_doc = self._get_closet_doc_from_docs(docs)\n",
    "        if closest_doc:\n",
    "            return [closest_doc]\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    # Only return closest doc\n",
    "    def _get_closet_doc_from_docs(self, docs):\n",
    "        # Return the doc having the min score below threshold\n",
    "        min_score = self.threshold\n",
    "        min_id = -1\n",
    "        for id, item in enumerate(docs):\n",
    "            doc, score = item\n",
    "            if min_score > score:\n",
    "                min_id = id\n",
    "                min_score = score\n",
    "        if min_score < self.threshold:\n",
    "            return docs[min_id][0]  # return doc\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "class Text_processor():\n",
    "    def __init__(self, folder_path, chunk_size=256, chunk_overlap=20):\n",
    "        self.folder_path = folder_path\n",
    "        self.folder_list = self._get_rescursive_folder()\n",
    "        self.doc_list = self._get_doc_path()\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.embeddings=OpenAIEmbeddings()\n",
    "        self.data = None\n",
    "        self.knowledge_base=None\n",
    "    \n",
    "    def _get_rescursive_folder(self):\n",
    "        folder_list = []\n",
    "        for folder in glob.iglob(f\"{self.folder_path}/**\"):\n",
    "            folder_list.append(folder)\n",
    "        return folder_list\n",
    "    \n",
    "    def _get_doc_path(self):\n",
    "        doc_list = []\n",
    "        for filename in glob.iglob(f'{self.folder_path}/**/*.md', recursive=True):\n",
    "            doc_list.append(filename)\n",
    "        return doc_list\n",
    "    \n",
    "    def _load_docs(self):\n",
    "        loader = DirectoryLoader(self.folder_path)\n",
    "        data = loader.load()\n",
    "        return data\n",
    "        \n",
    "    def _split_docs(self):\n",
    "        # Split the text into chunks using Langchain's CharacterTextSplitter\n",
    "        text_splitter = MarkdownTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "        upf_splits = text_splitter.split_documents(self.data)\n",
    "        return upf_splits\n",
    "    \n",
    "    def embed_docs(self):\n",
    "        # Load docs\n",
    "        self.data = self._load_docs()\n",
    "        \n",
    "        # Split docs\n",
    "        self.upf_splits = self._split_docs()\n",
    "        \n",
    "        # Filter chunks shorter than 1 sentence or 10 words\n",
    "        self.upf_splits = self._filter_chunk()\n",
    "        \n",
    "        # Embed chunks\n",
    "        self.knowledge_base = Knowledge_base(FAISS.from_documents(documents=self.upf_splits, embedding=self.embeddings))\n",
    "        return self.knowledge_base\n",
    "\n",
    "    def _filter_chunk(self):\n",
    "        filtered_upf_splits = []\n",
    "        for chunk in self.upf_splits:\n",
    "            if len(chunk.page_content.split()) > 5:\n",
    "                filtered_upf_splits.append(chunk)\n",
    "        return filtered_upf_splits\n",
    "    \n",
    "    def _get_doc_len(self, text, threshold_char=10):\n",
    "        # Check if the split is longer than 1 sentences\n",
    "        return len(text.split()) >= threshold_char\n",
    "\n",
    "    def get_n_doc(self):\n",
    "        return len(self.doc_list)\n",
    "\n",
    "    def save_knowledge_base(self, output_path):\n",
    "        self.knowledge_base.knowledge_base.save_local(output_path)\n",
    "    \n",
    "    def load_knowledge_base(self, input_path):\n",
    "        self.knowledge_base = Knowledge_base(FAISS.load_local(input_path, self.embeddings))\n",
    "        return self.knowledge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Text_processor(folder_path=\"../content/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = test.load_knowledge_base(\"knowledge_base_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.7,max_tokens=100)\n",
    "chain = load_qa_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(chain, knowledge_base, question):\n",
    "    doc = knowledge_base.search_from_knowledge_base(question)\n",
    "    return chain.run(input_documents=doc,question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ultra-processed food refers to food products that undergo extensive industrial processing and contain many additives, such as preservatives, artificial flavors, and colors. These foods often have high levels of added sugars, unhealthy fats, and sodium. They are typically packaged and ready-to-eat or require minimal preparation. Some examples of ultra-processed food include soda, packaged snacks, frozen meals, and fast food.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(chain, knowledge_base, \"What is ultra processed food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The types of food mentioned in the context are unprocessed or minimally processed foods, processed culinary ingredients, processed foods, and ultra-processed food and drink products.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_answer(chain, knowledge_base, \"what are the types of food?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
